{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOMmpDo5HWwXUZgCvTYVTNC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akshranair/Dive-into_deep-learning-pytorch/blob/main/Chapter_5_Deep_learning_Computation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3fqYNd3UpRoI"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Layers and Block"
      ],
      "metadata": {
        "id": "VhwNu_Xmuf5y"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "network = nn.Sequential(\n",
        "    nn.Linear(20,256),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(256,10),\n",
        ")"
      ],
      "metadata": {
        "id": "WSnZPePDpVHf"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = network(torch.rand(2,20))\n",
        "X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8Uk9ORPqsyo",
        "outputId": "22827628-bdc0-4469-9003-2d480926d2ed"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.0430,  0.1519,  0.0701, -0.1114, -0.0419, -0.0949, -0.0356,  0.0704,\n",
              "          0.0622,  0.1425],\n",
              "        [-0.0248,  0.1060, -0.0591, -0.0214,  0.0358, -0.0436, -0.2121, -0.0415,\n",
              "          0.2312,  0.1527]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#My Custom Block"
      ],
      "metadata": {
        "id": "_U1lPricukDC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "class MLP(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(MLP,self).__init__()\n",
        "    self.hidden = nn.Linear(20,256)\n",
        "    self.ReLU = nn.ReLU()\n",
        "    self.out = nn.Linear(256,10)\n",
        "\n",
        "  def forward(self,X):\n",
        "    X = self.hidden(X)\n",
        "    X = self.ReLU(X)\n",
        "    X = self.out(X)\n",
        "    return X\n",
        "\n",
        "network = MLP()\n",
        "input_tensor = torch.rand(2, 20)\n",
        "output = network.forward(input_tensor)\n"
      ],
      "metadata": {
        "id": "IkKRylwBskqO"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP, self).__init__()\n",
        "        self.hidden = nn.Linear(20, 256)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.out = nn.Linear(256, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.hidden(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.out(x)\n",
        "        return x\n",
        "\n",
        "network = MLP()\n",
        "input_tensor = torch.rand(2, 20)\n",
        "output = network.forward(input_tensor)\n"
      ],
      "metadata": {
        "id": "kPX8-C2HuVvb"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ExecutingCodeintheForwardPropagationFunction\n",
        "'''Sometimes, however, we might want to incorporate terms that are neither the result of \n",
        "previous layers nor updatable parameters. We call these con- stantparameters. \n",
        "Say for example that we want a layer that calculates the functionf(x,w) = c·w⊤x,\n",
        " where x is the input, w is our parameter, and c is some specified constant \n",
        " that is not updated dur- ing optimization. So we implement a FixedHiddenMLP class as follows.'''\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "FkIV1BU5ut7c",
        "outputId": "72d5a0db-fdc6-41b5-bb2d-035eba20d0fa"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Sometimes, however, we might want to incorporate terms that are neither the result of previous layers nor updatable parameters. We call these con- stantparameters. Sayforexamplethatwewantalayerthatcalculatesthefunctionf(x,w) = c·w⊤x, where x is the input, w is our parameter, and c is some specified constant that is not updated dur- ing optimization. So we implement a FixedHiddenMLP class as follows.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FixedHiddenMLP(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(FixedHiddenMLP,self).__init__()\n",
        "    self.rand_weight = nn.Parameter(torch.rand(20,20))\n",
        "    self.Dense = nn.Linear(20,20)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.Dense(x)\n",
        "    x = torch.relu(torch.matmul(x,self.rand_weight)+1)\n",
        "    x = self.Dense(x)\n",
        "    while torch.abs(x).sum()>1:\n",
        "      x/=2\n",
        "    return x"
      ],
      "metadata": {
        "id": "t06DD9hivb7m"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fhm = FixedHiddenMLP()\n",
        "fhm(torch.rand(20,20))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0UHU7hqc5T7N",
        "outputId": "9cc53564-aec1-4f64-f960-3749b65ed38f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-2.9307e-04,  1.9470e-03,  3.6472e-03, -2.6838e-03,  1.9587e-03,\n",
              "          4.0392e-03, -1.7272e-03, -1.9744e-03,  1.4534e-03, -4.1321e-03,\n",
              "          1.5687e-03, -6.4703e-05,  2.1209e-05, -1.2994e-03, -1.4937e-03,\n",
              "         -4.0552e-03, -8.8664e-04,  1.2365e-03, -3.2226e-04,  8.1781e-04],\n",
              "        [-5.7042e-03, -2.1495e-04,  2.1949e-03, -3.7988e-03,  3.5362e-03,\n",
              "          2.9682e-03, -4.6146e-04, -5.4571e-03, -1.2860e-03, -8.4555e-05,\n",
              "         -3.5694e-03, -2.2620e-04, -7.0694e-04, -2.0627e-03, -1.5064e-03,\n",
              "         -2.3406e-03, -9.5803e-04, -1.9322e-03, -4.6669e-04,  2.8688e-04],\n",
              "        [-3.1805e-03, -1.1292e-03,  1.6728e-03, -2.0966e-03,  3.6151e-03,\n",
              "          3.9539e-03,  6.5123e-04, -2.3592e-03, -1.2376e-03, -3.1776e-03,\n",
              "         -2.5913e-04, -1.7810e-03, -3.6920e-04, -9.1140e-04, -3.0957e-03,\n",
              "         -3.6990e-03, -7.1634e-05, -6.4787e-04, -1.7925e-03,  1.5855e-03],\n",
              "        [-1.1041e-03,  2.4173e-03,  2.8973e-03, -3.9454e-03,  1.5358e-03,\n",
              "          2.5152e-03, -1.4418e-03, -2.6756e-03,  5.7313e-04, -2.4187e-03,\n",
              "          2.2800e-04, -7.7852e-04, -3.6937e-04, -1.5477e-03, -1.3079e-03,\n",
              "         -4.4296e-03, -1.3858e-03,  1.1165e-03,  3.8677e-04,  4.9878e-04],\n",
              "        [-1.4703e-03,  1.6774e-03,  2.3501e-03, -3.8989e-03,  1.7016e-03,\n",
              "          3.5880e-03, -1.1527e-03, -5.4413e-03, -9.9902e-04, -2.3926e-03,\n",
              "         -9.1780e-04, -4.7009e-04, -1.1440e-03, -5.0884e-04, -1.5148e-03,\n",
              "         -2.9304e-03, -6.6320e-04, -8.9484e-04, -9.5304e-04,  6.7959e-05],\n",
              "        [-4.2006e-04,  1.2857e-03,  2.9957e-03, -3.4997e-03,  2.5150e-03,\n",
              "          3.5029e-03, -2.5379e-03, -1.1867e-03,  1.2948e-03, -3.2139e-03,\n",
              "          2.3770e-03, -3.2104e-04,  9.5446e-04, -4.5502e-04, -1.9495e-03,\n",
              "         -3.0726e-03, -1.4709e-03,  1.6132e-03, -1.9222e-04,  2.7366e-04],\n",
              "        [-2.1392e-03,  1.7141e-03,  2.4989e-03, -4.1406e-03,  2.7444e-03,\n",
              "          4.6234e-03, -2.9582e-03, -2.7089e-03, -4.7931e-04, -2.4343e-03,\n",
              "          7.9100e-04, -1.4801e-03,  2.9539e-03, -8.4079e-04, -1.6999e-03,\n",
              "         -2.3448e-03, -2.5098e-03,  1.5821e-03, -1.3707e-03,  2.2731e-04],\n",
              "        [-4.5112e-03,  1.0305e-03,  2.7314e-03, -4.3190e-03,  2.8627e-03,\n",
              "          3.1854e-03,  1.6756e-04, -4.2147e-03, -2.0809e-04, -4.6322e-04,\n",
              "         -2.4667e-03, -1.6329e-03,  9.0454e-05, -1.5652e-03, -1.0576e-03,\n",
              "         -1.6138e-03,  7.1413e-06,  3.4874e-04, -1.4587e-03,  7.3097e-04],\n",
              "        [-8.5380e-04,  1.4228e-03,  2.8096e-03, -3.5211e-03,  2.4994e-03,\n",
              "          4.1540e-03, -2.6453e-03, -1.9206e-03,  5.7837e-04, -3.1737e-03,\n",
              "          1.9178e-03, -5.4617e-04,  1.2868e-03, -5.4689e-04, -1.8305e-03,\n",
              "         -2.9178e-03, -1.4352e-03,  1.5154e-03, -5.0953e-04,  3.4184e-04],\n",
              "        [-3.8461e-04,  1.2963e-03,  3.0210e-03, -3.4799e-03,  2.4904e-03,\n",
              "          3.4744e-03, -2.5421e-03, -1.1447e-03,  1.3174e-03, -3.2273e-03,\n",
              "          2.4077e-03, -2.9962e-04,  9.1614e-04, -4.6921e-04, -1.9803e-03,\n",
              "         -3.0955e-03, -1.4541e-03,  1.6066e-03, -1.7312e-04,  3.0160e-04],\n",
              "        [-4.3964e-03,  3.6143e-03,  4.6720e-03, -4.2912e-03,  5.1175e-03,\n",
              "          5.5754e-03, -1.9028e-03, -1.5782e-03, -2.3945e-03, -1.8596e-03,\n",
              "         -4.9895e-03, -2.3429e-03,  5.0379e-03, -2.8407e-03, -2.2781e-03,\n",
              "         -4.0034e-03, -1.4118e-03,  1.8260e-03, -1.6895e-04,  3.1109e-04],\n",
              "        [-9.6884e-03,  1.6569e-03,  1.9648e-03, -7.8585e-03,  4.7211e-03,\n",
              "          4.2158e-03, -1.9606e-03, -6.9212e-03, -4.8129e-03,  6.1452e-03,\n",
              "         -5.6228e-03, -1.4612e-03,  2.4717e-03, -2.5672e-03, -1.0903e-03,\n",
              "         -5.2120e-04, -1.8631e-03, -3.3576e-03,  2.2154e-03, -3.2989e-03],\n",
              "        [-8.2093e-03,  6.3183e-04,  9.9704e-04, -6.9305e-03,  3.8650e-03,\n",
              "          2.4116e-03, -4.0081e-03, -6.5889e-03, -3.6825e-03,  4.3363e-03,\n",
              "         -2.0348e-03,  3.7993e-04, -1.0347e-04, -2.4264e-03, -8.6735e-04,\n",
              "         -2.4186e-03, -3.9354e-03, -2.1244e-03,  2.7871e-03, -1.8110e-03],\n",
              "        [-1.1937e-02, -2.4222e-03, -1.1543e-04, -6.8213e-03,  3.2407e-03,\n",
              "          1.0964e-03, -2.9987e-03, -8.4779e-03, -6.6664e-03,  6.5688e-03,\n",
              "         -1.3871e-03,  2.7435e-03, -1.5989e-03, -2.3644e-03, -2.4361e-03,\n",
              "          7.4387e-04, -4.9224e-03, -6.5687e-03,  8.4864e-04,  9.4197e-06],\n",
              "        [-2.8392e-04,  1.6611e-03,  3.0419e-03, -3.4482e-03,  2.0095e-03,\n",
              "          3.2466e-03, -2.2723e-03, -1.5188e-03,  1.1613e-03, -3.1355e-03,\n",
              "          2.0329e-03, -3.4128e-04,  6.2976e-04, -7.3957e-04, -2.0474e-03,\n",
              "         -3.3602e-03, -1.7260e-03,  1.4788e-03, -1.6459e-04,  3.1066e-04],\n",
              "        [-3.1923e-03,  2.5723e-03,  2.9355e-03, -4.0511e-03,  2.6779e-03,\n",
              "          3.0243e-03, -3.0802e-03, -3.4816e-03, -5.3960e-04, -1.4870e-04,\n",
              "          7.9013e-04, -6.5933e-04,  1.9826e-03, -1.4302e-03, -2.2610e-03,\n",
              "         -2.1389e-03, -2.2571e-03,  2.4295e-04,  6.4787e-04, -4.4049e-04],\n",
              "        [-3.8461e-04,  1.2963e-03,  3.0210e-03, -3.4799e-03,  2.4904e-03,\n",
              "          3.4744e-03, -2.5421e-03, -1.1447e-03,  1.3174e-03, -3.2273e-03,\n",
              "          2.4077e-03, -2.9962e-04,  9.1614e-04, -4.6921e-04, -1.9803e-03,\n",
              "         -3.0955e-03, -1.4541e-03,  1.6066e-03, -1.7312e-04,  3.0160e-04],\n",
              "        [-1.0813e-03,  1.9186e-03,  3.0159e-03, -3.6986e-03,  2.0913e-03,\n",
              "          3.6655e-03, -2.8549e-03, -1.6839e-03,  3.6148e-04, -2.6984e-03,\n",
              "          1.6078e-03, -8.9801e-04,  1.8069e-03, -1.0893e-03, -2.3094e-03,\n",
              "         -2.9926e-03, -2.2180e-03,  1.5264e-03, -7.0088e-04,  6.2930e-04],\n",
              "        [-2.7987e-03,  1.3994e-03,  9.5496e-04, -4.5835e-03,  2.0410e-03,\n",
              "          6.0499e-03, -2.9906e-03, -8.0336e-03, -3.3315e-03, -2.1616e-04,\n",
              "         -5.7975e-04,  9.6403e-05, -8.3960e-04, -3.4453e-04, -1.3873e-03,\n",
              "         -1.6034e-03, -7.0069e-04, -1.1567e-03,  6.2137e-04, -1.5750e-03],\n",
              "        [-1.1675e-02,  1.5542e-03,  3.6677e-03, -6.6023e-03,  5.4355e-03,\n",
              "          3.9875e-03, -4.1845e-03, -5.2017e-03, -3.2339e-03,  6.7987e-03,\n",
              "         -4.7624e-03, -2.3312e-03,  2.5628e-03, -4.9268e-03, -2.1125e-03,\n",
              "         -1.0381e-03, -6.2215e-04, -2.2943e-03,  1.5179e-03, -9.0960e-04]],\n",
              "       grad_fn=<DivBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wytgLj9u5dq6"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-mPXshr-5q6Q"
      },
      "execution_count": 10,
      "outputs": []
    }
  ]
}